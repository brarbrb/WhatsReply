{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VXCah5PMyOmk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from io import StringIO\n",
        "from dateutil import parser as date_parser\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fNKQtuExnyYu"
      },
      "outputs": [],
      "source": [
        "def parse_chat_log(file_path):\n",
        "    \"\"\"\n",
        "    Reads a chat log file and robustly parses each message line into structured data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            chat_content = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found. Please ensure the file is present.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    pattern = re.compile(\n",
        "        r'^(?:.*?)\\s*\\[(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}:\\d{2})\\s*(?:[AP]M)?\\]\\s*(.+?):\\s*(.*)',\n",
        "        re.IGNORECASE | re.MULTILINE\n",
        "    )\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for line in StringIO(chat_content):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        match = pattern.match(line)\n",
        "\n",
        "        if match:\n",
        "            date_str, time_str, speaker, message = match.groups()\n",
        "\n",
        "            clean_speaker = re.sub(r'\\[.*?\\]|\\(|\\)|\\u200e|\\u200f', '', speaker).strip()\n",
        "\n",
        "            data.append({\n",
        "                'Date': date_str,\n",
        "                'Time': time_str,\n",
        "                'Speaker': clean_speaker,\n",
        "                'Message': message.strip()\n",
        "            })\n",
        "        elif data:\n",
        "            data[-1]['Message'] += ' ' + line\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def transform_to_sql_schema(df, user_map=None):\n",
        "    \"\"\"\n",
        "    Transforms the raw DataFrame into the required 'messages' table schema.\n",
        "\n",
        "    Behavior:\n",
        "    - If `user_map` is None, builds a deterministic mapping from original names to\n",
        "      internal ids `u_1, u_2, ...` used for `conv_id` and receiver pairing.\n",
        "    - The final returned DataFrame uses `sender_user_id` = original sender name\n",
        "      and `receiver_user_id` = original receiver name as well, while internal\n",
        "      `u_*` ids are used internally for conv logic.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Build a user_map from speakers if not supplied.\n",
        "    speakers = list(df['Speaker'].dropna().unique())\n",
        "    if user_map is None:\n",
        "        # Sort for determinism across runs\n",
        "        speakers_sorted = sorted(speakers)\n",
        "        user_map = {s: f\"u_{i+1}\" for i, s in enumerate(speakers_sorted)}\n",
        "\n",
        "    # Keep original speaker names\n",
        "    df['speaker_name'] = df['Speaker']\n",
        "    # Internal mapped id used for conv_id/receiver logic\n",
        "    df['sender_internal'] = df['Speaker'].map(user_map)\n",
        "\n",
        "    # For two-person chats, map each sender_internal to the other as receiver_internal.\n",
        "    unique_sender_ids = list(df['sender_internal'].dropna().unique())\n",
        "    if len(unique_sender_ids) == 2:\n",
        "        receiver_internal_map = {unique_sender_ids[0]: unique_sender_ids[1],\n",
        "                                 unique_sender_ids[1]: unique_sender_ids[0]}\n",
        "    else:\n",
        "        # For group chats or single-participant logs, set receiver_internal to None.\n",
        "        receiver_internal_map = {uid: None for uid in unique_sender_ids}\n",
        "\n",
        "    df['receiver_internal'] = df['sender_internal'].map(receiver_internal_map)\n",
        "\n",
        "    def generate_id_and_timestamp(row):\n",
        "        msg_id = str(uuid.uuid4())\n",
        "        try:\n",
        "            dt_obj = date_parser.parse(f\"{row['Date']} {row['Time']}\", dayfirst=True)\n",
        "            sent_at = dt_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            dt_obj = pd.NaT\n",
        "            sent_at = None\n",
        "        return pd.Series([msg_id, sent_at, dt_obj])\n",
        "\n",
        "    df[['msg_id', 'sent_at','dt_object']] = df.apply(generate_id_and_timestamp, axis=1)\n",
        "\n",
        "    # Use the (sorted) internal user ids from the map to create a deterministic conv_id\n",
        "    user_ids = sorted(set(user_map.values()))\n",
        "    df['conv_id'] = 'chat:' + '_'.join(user_ids) if user_ids else None\n",
        "\n",
        "    df = df.sort_values(by='dt_object').reset_index(drop=True)\n",
        "    df = df.rename(columns={'Message': 'text'})\n",
        "    df = df.sort_values(['conv_id', 'sent_at']).reset_index(drop=True)\n",
        "\n",
        "    block_id = (\n",
        "        (df['conv_id'] != df['conv_id'].shift()) |\n",
        "        (df['sender_internal'] != df['sender_internal'].shift())\n",
        "    ).cumsum()\n",
        "\n",
        "    df['block_id'] = block_id\n",
        "    df_merged = (\n",
        "        df.groupby('block_id', as_index=False)\n",
        "          .agg({\n",
        "              'msg_id': 'first',\n",
        "              'conv_id': 'first',\n",
        "              'sender_internal': 'first',\n",
        "              'receiver_internal': 'first',\n",
        "              'sent_at': 'min',\n",
        "              'text': lambda x: \"\\n\".join(x),\n",
        "          })\n",
        "    )\n",
        "    df_merged = df_merged.drop(columns=['block_id'])\n",
        "    df_merged = df_merged.sort_values(by='sent_at').reset_index(drop=True)\n",
        "\n",
        "    # conv_turn calculation: increment whenever the sender_internal is same as previous receiver_internal\n",
        "    df_merged['conv_turn'] = (df_merged['sender_internal'] == df_merged['receiver_internal'].shift()).cumsum() + 1\n",
        "\n",
        "    df_merged['next_text'] = df_merged['text'].shift(-1)\n",
        "    df_merged['next_sender_internal'] = df_merged['sender_internal'].shift(-1)\n",
        "    df_merged['answer'] = df_merged['next_text']\n",
        "\n",
        "    # Map internal ids back to original names for the final `sender_user_id` and `receiver_user_id` columns\n",
        "    reverse_map = {v: k for k, v in user_map.items()}\n",
        "    df_merged['sender_user_id'] = df_merged['sender_internal'].map(reverse_map)\n",
        "    df_merged['receiver_user_id'] = df_merged['receiver_internal'].map(reverse_map)\n",
        "\n",
        "    # Drop helper columns\n",
        "    df_merged = df_merged.drop(columns=['next_text', 'next_sender_internal', 'sender_internal', 'receiver_internal'])\n",
        "\n",
        "    df_merged['answer'] = df_merged['answer'].fillna('<EOC>') #END OF CONVERSATION\n",
        "\n",
        "    final_columns = [\n",
        "        'msg_id',\n",
        "        'conv_id',\n",
        "        'conv_turn',\n",
        "        'sender_user_id',\n",
        "        'receiver_user_id',\n",
        "        'sent_at',\n",
        "        'text',\n",
        "        'answer'\n",
        "    ]\n",
        "\n",
        "    final_df = df_merged[final_columns]\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process every .txt chat file in the raw data directory and concatenate results\n",
        "from pathlib import Path\n",
        "RAW_DATA_DIR = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\RAG\\RAG_data\\raw_data\"\n",
        "all_txt_files = sorted(Path(RAW_DATA_DIR).glob('*.txt'))\n",
        "\n",
        "dfs = []\n",
        "mapping_rows = []\n",
        "for p in all_txt_files:\n",
        "    df_raw = parse_chat_log(str(p))\n",
        "    if df_raw.empty:\n",
        "        continue\n",
        "\n",
        "    # Build a deterministic user_map per chat from speakers\n",
        "    speakers = list(df_raw['Speaker'].dropna().unique())\n",
        "    speakers_sorted = sorted(speakers)\n",
        "    user_map_local = {s: f\"u_{i+1}\" for i, s in enumerate(speakers_sorted)}\n",
        "\n",
        "    # Transform using the local mapping\n",
        "    df_trans = transform_to_sql_schema(df_raw, user_map=user_map_local)\n",
        "    if not df_trans.empty:\n",
        "        df_trans['source_file'] = p.name\n",
        "        dfs.append(df_trans)\n",
        "\n",
        "    # Build mapping rows to save later\n",
        "    user_ids_sorted = sorted(set(user_map_local.values()))\n",
        "    conv_id_local = 'chat:' + '_'.join(user_ids_sorted) if user_ids_sorted else None\n",
        "    for orig, uid in user_map_local.items():\n",
        "        mapping_rows.append({\n",
        "            'conv_id': conv_id_local,\n",
        "            'chat_file': p.name,\n",
        "            'original_name': orig,\n",
        "            'user_id': uid\n",
        "        })\n",
        "\n",
        "if dfs:\n",
        "    df_messages = pd.concat(dfs, ignore_index=True)\n",
        "else:\n",
        "    df_messages = pd.DataFrame()\n",
        "\n",
        "# Create mapping DataFrame and remove duplicates\n",
        "if mapping_rows:\n",
        "    df_mappings = pd.DataFrame(mapping_rows)\n",
        "    df_mappings = df_mappings.drop_duplicates().reset_index(drop=True)\n",
        "else:\n",
        "    df_mappings = pd.DataFrame(columns=['conv_id', 'chat_file', 'original_name', 'user_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "82BA8o3CxcH4",
        "outputId": "bf254d6d-76f4-4ec5-b899-63b130575ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote combined CSV to: C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\RAG\\RAG_data\\KB_data.csv\n",
            "Wrote sender mappings to: C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\RAG\\RAG_data\\user_mappings.csv\n"
          ]
        }
      ],
      "source": [
        "# Write combined DataFrame to KB_data.csv in the RAG_data folder and save mappings\n",
        "from pathlib import Path\n",
        "output_dir = Path(RAW_DATA_DIR).parent\n",
        "output_csv = output_dir / 'KB_data.csv'\n",
        "mapping_csv = output_dir / 'user_mappings.csv'\n",
        "\n",
        "if not df_messages.empty:\n",
        "    df_messages.to_csv(str(output_csv), index=False)\n",
        "    print(f\"Wrote combined CSV to: {output_csv}\")\n",
        "else:\n",
        "    print(\"No messages were processed; no CSV written.\")\n",
        "\n",
        "# Save sender name mappings for each chat\n",
        "if not df_mappings.empty:\n",
        "    df_mappings.to_csv(str(mapping_csv), index=False)\n",
        "    print(f\"Wrote sender mappings to: {mapping_csv}\")\n",
        "else:\n",
        "    print(\"No sender mappings to write.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
